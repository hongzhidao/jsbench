# An nginx Engineer Built a Real Project with AI

> Without writing a single line of code by hand. The entire process was discussion, direction-giving, review, and more discussion. This is the full story.

I work on the nginx team. Recently I did something: had AI write a complete project from scratch, then — without writing any code myself — refactored its entire architecture purely by guiding AI.

I wrote ten articles about this process, mostly about architecture. This is the complete story.

## Why I Did This

Benchmarking tools are something every backend engineer has used. wrk has great performance but uses Lua for scripting. k6 uses JavaScript but weighs dozens of megabytes and requires learning a custom API. What I wanted was simple: wrk's performance, the standard JS fetch API, zero learning curve.

This kind of project is perfect for AI — clear requirements, well-defined tech stack, manageable scope. But I had an ulterior motive: **I wanted to see what code AI actually produces on its own.** So I didn't give it any architectural guidance upfront. I let it work freely.

## After AI Finished, I Was Surprised Twice

First surprise: **It actually worked.**

Two thousand lines of C code — compiled, ran, tests passed. Naming was accurate, prefixes consistent — across thirteen source files and over a hundred functions, not a single violation. In C, a language without namespaces, this level of discipline beats many human projects.

Second surprise: **After looking at the whole thing, I couldn't pinpoint what was wrong, but something felt off.**

Each function looked fine in isolation. But the relationships between files, the boundaries between modules, the direction of dependencies — there was no coherent design. Everything worked, but nothing felt *designed* together. It felt *piled* together.

Later I figured it out: **AI's code is like a house built without blueprints — every wall is well-constructed, but you get lost walking through it.** That's what "code without architecture" feels like.

So I started refactoring. One rule: **no writing code by hand.** Every change was made through discussion with AI — I raised problems, gave direction, reviewed results, corrected details.

## When the Direction Is Right, AI's Execution Is Remarkable

The first real problem during refactoring: fetch didn't support concurrency. Three 300ms requests sent with `Promise.all` should take 300ms total, but actually took 905ms — they ran sequentially. AI had used "pseudo-async": the function signature returned a Promise, but internally it blocked synchronously.

I told it the right approach — a global event loop with pending Promises — **and it got it right in one pass across 9 files. 905ms → 302ms.**

If I had only said "fetch has a bug," AI would probably have patched within the existing architecture. Because I gave it a clear architectural direction, it nailed it in one shot.

**It wasn't AI's capability that changed — it was the direction I gave.** Same AI, different direction, vastly different output. This is the single most important lesson from the entire journey.

## The Most Dangerous Bug: Everything Looked Fine

But right after that came the most unsettling discovery.

After fixing concurrency, all tests passed. I continued reviewing and found that in bench mode, fetch couldn't even run in worker threads. **Every single request was failing.**

But the test report said: **16,576 requests, 0 errors, passed.**

The reason: the code unconditionally counted every call as "successful." The tests only checked this number.

**AI's code didn't check for errors. AI's tests didn't verify errors. They shared the same blind spot — perfectly complementing each other to create the illusion that everything was fine.**

This wasn't coincidence. Implementation and tests were generated by the same model, with consistent blind spots. You couldn't see the problem from either side. **Only a human reviewing from outside could break the loop.**

I told AI where the problem was, and it fixed it in minutes. Finding the problem was the human's job. Fixing it was AI's. This division of labor ran throughout the entire journey.

## The Power of One Sentence

The deeper the refactoring went, the more I noticed a striking contrast.

Once, I wanted to encapsulate the event engine into a clean interface. AI got stuck — not because it couldn't write code, but because it didn't know which direction to take. Several problems were entangled. It could see each one, but its solutions either sidestepped them or just moved them elsewhere. After several rounds, no progress.

Then I said one sentence: **"Each thread has exactly one epoll."**

Just that. AI immediately knew what to do — use thread-local storage, rewrite all epoll interfaces, remove the fd parameter from every call site. Over a dozen changes scattered across different files, done in one pass, zero compiler warnings.

This happened repeatedly. I said "this field doesn't belong in conn, it belongs to a new concept called http_peer" — AI created the new struct, moved fields, updated every reference. Six files, twenty-plus changes, not one missed. I said "combine epoll and timers into a work engine" — it modified over a dozen files, aligned signatures, updated every caller.

When humans make changes across twenty locations, they'll inevitably miss one. AI doesn't.

**But every time, the prerequisite was the same: I gave it an accurate judgment.** Without that sentence, AI spun in place. With it, AI executed faster and more reliably than any human engineer could.

This helped me understand something: **AI has architectural knowledge but lacks architectural judgment.** It knows all the design patterns and techniques. But facing tangled code, it won't say "fix this first — that enables the next step." It tends to list options and leave the choice to you.

**One judgment from you, and AI applies it to every line of code across dozens of files. The more accurate the judgment, the greater the leverage.**

## What I Learned from AI

This wasn't entirely a one-way process. Sometimes AI's approach made me rethink my own habits.

Take naming — a famously hard problem in programming. But AI names things almost without hesitation. A model that processes natural language finds naming variables and functions to be one of its easiest tasks. This made me realize: naming is hard for humans not because it's inherently difficult, but because the human brain is juggling too many things at once, and naming gets squeezed out of attention.

Or consistency. Over a hundred functions, not a single prefix violation. Humans writing this much code inevitably have off days. AI doesn't. **For mechanical consistency, AI is naturally more reliable than humans.**

There's a deeper insight too. AI's code lacks architecture, but each function's implementation is *direct* — no showing off, no over-abstraction, just the most straightforward way to get things done. Sometimes I thought a piece of code could be written "more elegantly," but on reflection, that "elegance" was probably just aesthetic preference, not genuine improvement. **AI forced me to distinguish "real architectural problems" from "personal aesthetic preferences."** The former must be fixed; the latter can be left alone. This distinction is crucial for architectural judgment.

## Overturning Yourself Is Normal Too

Once I overturned my own previous design. In the previous step, I had just made a parameter into a thread-level implicit variable; in the next step, I changed it back to explicit passing — because a new requirement emerged and a better abstraction appeared.

**The previous decision wasn't wrong. As the system evolved, a better design emerged.** Architecture isn't built in one shot — you make the most reasonable choice at each step, ready to adjust when new understanding arrives.

AI has zero complaints about overturning previous work. It wrote the code last time; it rewrites it this time. **Humans suffer from sunk cost psychology. AI doesn't.** This is especially useful during refactoring — you can always say "the previous approach isn't good enough, let's try another," and AI has no resistance whatsoever.

## What's Missing Isn't Features — It's Concepts

The final phase returned to the most fundamental question: **what was missing from the code?**

Not features — all features worked. What was missing were concepts.

Why was request timing scattered across the connection struct? Because the code had no concept of "one HTTP exchange." Why did one function take six loose parameters? Because the code had no concept of "one fetch operation."

**When code feels wrong but you can't articulate why, it's often because some concept should exist but hasn't been expressed yet.** Give it a name, give it a struct, and many problems dissolve on their own.

AI won't proactively discover missing concepts. It puts data wherever is most convenient — wherever something is needed, that's where it goes. But "convenient" and "correct" aren't the same thing.

Once you tell it what's missing, though, AI creates structs, moves fields, and updates every reference with speed and accuracy far beyond what humans can achieve.

## Practical Advice

**Use the strongest model.** AI is fundamentally an intelligent tool — intelligence level directly determines output quality. Architecture-level changes require understanding context, maintaining cross-file consistency, and making reasonable implementation choices. The stronger the model, the better it performs. I used the strongest available throughout, and the difference was substantial.

**Use manual confirmation mode for refactoring.** AI agents typically offer fully automatic and manual confirmation modes. Refactoring involves architectural judgment and often requires mid-course direction changes — you set out to change A and discover you need to change B first. In automatic mode, AI barrels down the wrong path. Manual confirmation lets you course-correct at every step.

**Let AI write tests, but review the tests themselves.** Testing remains the best way to ensure code quality. The key is making sure tests verify the right things — not just "it runs," but "the results are correct."

**Practice, practice, practice.** There are no shortcuts for architectural ability. It's not about writing more code — it's about looking back afterward: is this structure really right? Do these fields really belong here? Where is this module's boundary? Now that AI has driven refactoring costs to near zero, **you can practice architecture more frequently than ever before.** Refactoring used to be expensive; now you just explain the direction to AI. Same amount of time, more cycles of "find problem → think → change → verify." Each cycle sharpens your judgment.

**Read good code.** My architectural judgment largely comes from reading source code of projects like nginx and njs. In well-designed codebases, a struct definition is itself documentation of a domain model. Watching how they draw conceptual boundaries is more direct than any design patterns book.

**Go deep in one domain.** AI is an amplifier. The deeper you are in a domain, the greater the leverage AI gives you. Pick a direction, spend years truly mastering it. Depth is the scarcest asset in the AI era.

**Work on professional projects, study high-quality code, and exchange ideas with skilled people.** My architectural judgment wasn't self-taught — it was forged working alongside top engineers on the nginx team. Watching how Igor Sysoev designs a red-black tree, abstracts an event model, or expresses a complete concept through a single struct — you can't learn these things from documentation alone. You have to immerse yourself in real projects. If you have the chance to contribute to high-quality open source projects or work with people stronger than you, that's the fastest growth path. AI can amplify ability, but the source of ability is people and projects.

## What This Journey Made Clear

Ten articles, each with corresponding code changes. Not a single line written by hand — all through guiding AI. Core architecture completely rebuilt, functionality unchanged, tests passing after every change.

The realizations that hit hardest:

**Writing code is no longer the bottleneck — managing complexity is.** AI can write two thousand lines of working code. But that code's complexity is out of control — it works, but it can't evolve. Managing complexity is architecture. In the AI era, architecture's value hasn't decreased — it's been **amplified.**

**Direction is the greatest lever.** How accurate your direction is determines how much leverage AI provides. Direction comes from experience and domain depth — I could judge that fetch should use an event loop, timers should use a red-black tree, and the connection layer shouldn't know about HTTP, because I've worked in this domain for years. **AI amplifies the abilities you already have. It doesn't create them from nothing.**

**Review is the last line of defense.** 16,576 requests all failing, report showing 0 errors — I won't forget this. AI-written code and AI-written tests share blind spots. Only a human can break the loop from the outside.

**Not writing code by hand is viable — and might be the better way to work.** I find the problem → think through the direction → tell AI → review the result → correct details. This loop lets me focus entirely on judgment rather than implementation. **Judgment is the core asset.**

## Complete Series

If you're interested in a specific topic:

- **What does AI-written code look like?** → Article 1
- **How to guide AI to improve architecture?** → Article 3
- **What is good architecture?** → Article 4
- **How to encapsulate an event engine?** → Article 5
- **How to choose data structures?** → Article 6
- **What does layering actually separate?** → Article 7
- **Methods for struct design** → Article 9
- **How to encapsulate complexity?** → Article 10

Complete ten-article series: "An nginx Engineer Took Over AI's Benchmark Tool"

---

GitHub: https://github.com/hongzhidao/jsbench
